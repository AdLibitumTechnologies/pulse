\chapter{State of the art} \label{chap:sota}

\section*{}

% Neste capítulo é descrito o estado da arte e são
% apresentados trabalhos relacionados para mostrar o que existe no
% mesmo domínio e quais os problemas em aberto.

% Deve deixar claro que existe uma oportunidade de desenvolvimento que
% cobre alguma falha concreta.

% O capítulo deve também efetuar uma revisão tecnológica às principais
% ferramentas utilizáveis no âmbito do projeto, justificando futuras
% escolhas.

% No final do capítulo deverá ser apresentado um resumo com as 
% principais conclusões que se podem tirar.

In this chapter, the background needed to understand the problem of 
heart rate detection using the \evm{} method is presented on 
section~\ref{sec:evm}. Before that, a similar research, which uses the 
\ica{} model to assess the cardiac pulse, is described in more 
detail on section~\ref{sec:relatedwork}. In addition, a description of the 
technologies to be used will be described on section~\ref{sec:technologies}.

\section{Related Work} \label{sec:relatedwork}

Successful attempts have already been made on real-time, contact-free 
heart rate assessment using a webcamera. In these attempts, two methods 
have been used: \ica{}, and \evm{}.

An \emph{iOS} application named \emph{Vital Signs Camera}, developed by 
\emph{Philips}, has also been able to detect a person's heart rate and 
breathing rate using the smartphone's camera~\cite{Philips2013}. 
However, as far as I know, there is no research work available to 
the public since its technology was developed internally by \emph{Philips}.

\subsection{\ica} \label{sec:ica}

\ica{} is a special case of \emph{blind source separation} and is a relatively 
new technique for uncovering independent signals from a set of observations 
that are composed of linear mixtures of the underlying 
sources~\cite{Comon1994Independent}.

In this case, the underlying source signal of interest is the cardiac pulse
that propagates throughout the body, which modify the path length of the 
incident ambient light due to volumetric changes in the facial blood vessels
during the cardiac cycle, such that subsequent changes in amount of reflected
light indicate the timing of cardiovascular events.

By recording a video of 
the facial region, the red, green, and blue (RGB) color sensors pick up a 
mixture of the reflected plethysmographic signal along with other sources of 
fluctuations in light due to artifacts. Each color sensor records a mixture 
of the original source signals with slightly different weights. These observed 
signals from the red, green and blue color sensors are denoted by $x_{1}(t)$,
$x_{2}(t)$ and $x_{3}(t)$ respectively, which are amplitudes of the recorded 
signals at time point $t$. In conventional \ica{} model the number of 
recoverable sources cannot exceed the number of observations, thus three 
underlying source signals were assumed, represented by $s_{1}(t)$, $s_{2}(t)$ 
and $s_{3}(t)$. The \ica{} model assumes that the observed signals are linear 
mixtures of the sources, i.e. $x_{i}(t) = \sum_{j=1}^{3} a_{ij} s_{j}(t)$ for 
each $i=1,2,3$. This can be represented compactly by the mixing equation

\begin{equation}
  x(t) = As(t)
\end{equation}

where the column vectors $x(t) = [x_{1}(t), x_{2}(t), x_{3}(t)]^{T}$, 
$s(t) = [s_{1}(t), s_{2}(t), s_{3}(t)]^{T}$ and the square $3\times3$ 
matrix $A$ contains the mixture coefficients $a_{ij}$. The aim of \ica{} model 
is to find a separating or demixing matrix $W$ that is an approximation of the 
inverse of the original mixing matrix $A$ whose output

\begin{equation}
  \hat{s}(t) = Wx(t)
\end{equation}

is an estimate of the vector $s(t)$ containing the underlying source signals. 
To uncover the independent sources, W must maximize the non-Gaussianity of 
each source. In practice, iterative methods are used to maximize or minimize 
a given cost function that measures non-Gaussianity~\cite{Poh2010Non, 
Poh2011Advancements}.

\section{\evm} \label{sec:evm}

In contrast to the \ica{} model that focus on extracting a single number,
the \evm{} uses localized spatial pooling and temporal filtering to extract
and reveal visually the signal corresponding to the cardiac pulse. This
allows for amplification and visualization of the heart rate signal at each 
location on the face. This creates potential for monitoring and diagnostic
applications to medicine, i.e. the asymmetry in facial blood flow can be a 
symptom of arterial problems.

Besides color amplification, the \evm{} method is also able to reveal 
low-amplitude motion which may be hard or impossible for humans to see.
Previous attempts to unveil imperceptible motions in videos have been 
made, such as, \cite{Liu2005Motion} which follows a \emph{Lagrangian}
perspective, as in fluid dynamics where the trajectory of particles
is tracked over time. By relying on accurate motion estimation and
additional techniques to produce good quality synthesis, such as, 
motion segmentation and image in-painting, the algorithm complexity 
and computation is expensive and difficult.

On the contrary, the \evm{} method is inspired by the \emph{Eulerian}
perspective, where properties of a voxel of fluid, such as pressure 
and velocity, evolve over time. The approach of this method to motion 
magnification is the exaggeration of motion by amplifying temporal 
color changes at fixed positions, instead of, explicitly estimation 
of motion.

This method approach, illustrated in figure~\ref{fig:evm}, combines 
spatial and temporal processing to emphasize subtle temporal changes 
in a video. First, the video sequence is decomposed into different 
spatial frequency bands. Because they may exhibit different 
signal-to-noise ratios, they may be magnified differently.
In the general case, the full Laplacian pyramid~\cite{Burt1983Laplacian}
may be computed. Then, temporal processing is performed on each 
spatial band. The temporal processing is uniform for all spatial 
bands, and for all pixels within each band. After that, the extracted
bandpass signal is magnified by a factor of $\alpha$, which can be 
specified by the user, and may be attenuated automatically. Finally, 
the magnified signal is added to the original and the spatial pyramid
collapsed to obtain the final output.

\fig{evm}{Overview of the \evm{} method.}

\subsection{Emphasize color variations for human pulse} \label{sec:evm-color}

The extraction of a person's cardiac pulse using the \evm{} method 
was demonstrated in~\cite{Wu2012Eulerian}. It was also presented that
using the right configuration can help extract the desired signal.
There are four steps to take then processing a video by \evm{}: 

\begin{enumerate}
  \item select a temporal bandpass filter;
  \item select an amplification factor, $\alpha$;
  \item select a spatial frequency cutoff (specified by spatial wavelength, 
        $\lambda_c$) beyond which an attenuated version of $\alpha$ is used;
  \item select the form of the attenuation for $\alpha$ —- either force 
        $\alpha$ to zero for all $\lambda < \lambda_c$, or linearly scale 
        $\alpha$ down to zero.
\end{enumerate}

For human pulse color variation, two temporal filters may be used, first
selecting frequencies within 0.4-4Hz, corresponding to 24-240 beats per 
minute (bpm), then a narrow band of 0.83-1Hz (50-60 bpm) may be used, 
if the extraction of the pulse rate was successful.

To emphasize the color change as much as possible, a large amplification 
factor, $\alpha \approx 100$, and spatial frequency cutoff, 
$\lambda_c \approx 1000$, is applied. With an attenuation of $\alpha$ to
zero for spatial wavelengths below $\lambda_c$.

The resulting output can be seen in figure~\ref{fig:evm-color}.

\fig{evm-color}{Emphasis of the face color changes using the \evm{} method.}

\section{Technologies} \label{sec:technologies}

Below are short descriptions of the main technologies that will be 
used during this research work.

\begin{description}

\item[Android SDK] \hfill 

\emph{Android SDK} is the development kit for the \emph{Android} 
platform. The \emph{Android} platform is an open source, Linux-based 
operating system, primarily designed for touchscreen mobile devices, 
such as, smartphones.

Because of its open source code and permissive licensing, it allows 
the software to be freely modified and distributed. This have allowed
\emph{Android} to be the software of choice for technology companies
who require a low-cost, customizable, and lightweight operating system 
for mobile devices and others.

\emph{Android} has also become the world's most widely used smartphone 
platform with a worldwide smartphone market share of 75\%
during the third quarter of 2012~\cite{Idc2013Android}.

\pagebreak

\item[OpenCV -- Computer Vision Library] \hfill 

\emph{OpenCV} is a library of programming functions mainly aimed at 
real-time image processing. To support these, it also includes a 
statistical machine learning library. Moreover, it is a cross-platform 
and open source library that is free to use and modify under the BSD 
license.

\begin{quote}
  ``OpenCV was built to provide a common infrastructure for computer 
  vision applications and to accelerate the use of machine perception 
  in the commercial products.''~\cite{Opencv2013About}
\end{quote}

\end{description}
