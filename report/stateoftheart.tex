\chapter{State of the art} \label{chap:sota}

\section*{}

% Neste capítulo é descrito o estado da arte e são
% apresentados trabalhos relacionados para mostrar o que existe no
% mesmo domínio e quais os problemas em aberto.

% Deve deixar claro que existe uma oportunidade de desenvolvimento que
% cobre alguma falha concreta.

% O capítulo deve também efetuar uma revisão tecnológica às principais
% ferramentas utilizáveis no âmbito do projeto, justificando futuras
% escolhas.

% No final do capítulo deverá ser apresentado um resumo com as
% principais conclusões que se podem tirar.

This chapter presents several studies regarding the heart rate estimation
from a person's face captured through a simple webcam.

Section~\ref{sec:sota:photo} describe the concept that explains how the
cardiac pulse is detected from a person's face in a remote, contact-free way.

Post-processing methods, which may be applied to the retrieved signal,
are detailed on section~\ref{sec:sota:post}.

In order to estimate the heart rate, a couple of techniques are also detailed
on section~\ref{sec:sota:estimation}.

Finally, section~\ref{sec:sota:tech} reviews the main
technologies and tools used throughout this work.

\section{Photo-plethysmography} \label{sec:sota:photo}

Photo-plethysmography~(PPG) is the concept of measuring volumetric changes
of an organ optically. Its most established use is in pulse oximeters.

PPG is based on the principle that blood absorbs more light than surrounding
tissue thus variations on blood volume affect light
reflectance~\cite{Verkruysse2008Remote}.

The use of dedicated light sources and infra-red wavelengths, and contact probes
has been the norm~\cite{Ulyanov1993Pulse, Greneker1997Radar, Garbey2007Contact}.
However, recently, remote, non-contact PPG imaging has been explored. % TODO references

The method used on the article~\cite{Verkruysse2008Remote} captures the pixel
values (red, green, and blue channels) of the facial area of a previously
recorded video where volunteers were asked to minimize movements. The pixel
values within a region of interest (ROI) was then averaged for each frame.
This spatial averaging was found to significantly increase signal-to-noise
ratio. The heart rate estimation was then calculated by applying Fast Fourier
transforms and the power spectrum as explained on
section~\ref{sec:sota:estimation:power}.

The authors of~\cite{Verkruysse2008Remote} demonstrate that the fact that
the green channel features a stronger heart rate signal as compared to the
red and blue channels, is a strong evidence that the signal is due to
variations in blood volume because (oxy-) hemoglobin absorbs green light.

\section{Signal post-processing} \label{sec:sota:post}

After obtaining the raw pixel values (red, green, and blue channels), a
conjunction of the following methods may be used to extract and improve the
reflected plethysmography signal. However, each method introduces complexity
and expensive computation.

\subsection{\ica} \label{sec:sota:post:ica}

\ica{} is a special case of \emph{blind source separation} and is a relatively
new technique for uncovering independent signals from a set of observations
that are composed of linear mixtures of the underlying
sources~\cite{Comon1994Independent}.

In this case, the underlying source signal of interest is the cardiac pulse
that propagates throughout the body, which modify the path length of the
incident ambient light due to volumetric changes in the facial blood vessels
during the cardiac cycle, such that subsequent changes in amount of reflected
light indicate the timing of cardiovascular events.

By recording a video of
the facial region, the red, green, and blue~(RGB) color sensors pick up a
mixture of the reflected plethysmographic signal along with other sources of
fluctuations in light due to artifacts. Each color sensor records a mixture
of the original source signals with slightly different weights. These observed
signals from the red, green and blue color sensors are denoted by $x_{1}(t)$,
$x_{2}(t)$ and $x_{3}(t)$ respectively, which are amplitudes of the recorded
signals at time point $t$. In conventional \ica{} model the number of
recoverable sources cannot exceed the number of observations, thus three
underlying source signals were assumed, represented by $s_{1}(t)$, $s_{2}(t)$
and $s_{3}(t)$. The \ica{} model assumes that the observed signals are linear
mixtures of the sources, i.e. $x_{i}(t) = \sum_{j=1}^{3} a_{ij} s_{j}(t)$ for
each $i=1,2,3$. This can be represented compactly by the mixing equation

\begin{equation}
  x(t) = As(t)
\end{equation}

where the column vectors $x(t) = [x_{1}(t), x_{2}(t), x_{3}(t)]^{T}$,
$s(t) = [s_{1}(t), s_{2}(t), s_{3}(t)]^{T}$ and the square $3\times3$
matrix $A$ contains the mixture coefficients $a_{ij}$. The aim of \ica{} model
is to find a separating or demixing matrix $W$ that is an approximation of the
inverse of the original mixing matrix $A$ whose output

\begin{equation}
  \hat{s}(t) = Wx(t)
\end{equation}

is an estimate of the vector $s(t)$ containing the underlying source signals.
To uncover the independent sources, W must maximize the non-Gaussianity of
each source. In practice, iterative methods are used to maximize or minimize
a given cost function that measures non-Gaussianity~\cite{Poh2010Non,
Poh2011Advancements}.

\subsection{\evm} \label{sec:sota:post:evm}

\fig{evm}{Overview of the \evm{} method.}

In contrast to the \ica{} model that focus on extracting a single number,
the \evm{} uses localized spatial pooling and temporal filtering to extract
and reveal visually the signal corresponding to the cardiac pulse. This
allows for amplification and visualization of the heart rate signal at each
location on the face. This creates potential for monitoring and diagnostic
applications to medicine, i.e. the asymmetry in facial blood flow can be a
symptom of arterial problems.

Besides color amplification, the \evm{} method is also able to reveal
low-amplitude motion which may be hard or impossible for humans to see.
Previous attempts to unveil imperceptible motions in videos have been
made, such as, \cite{Liu2005Motion} which follows a \emph{Lagrangian}
perspective, as in fluid dynamics where the trajectory of particles
is tracked over time. By relying on accurate motion estimation and
additional techniques to produce good quality synthesis, such as,
motion segmentation and image in-painting, the algorithm complexity
and computation is expensive and difficult.

On the contrary, the \evm{} method is inspired by the \emph{Eulerian}
perspective, where properties of a voxel of fluid, such as pressure
and velocity, evolve over time. The approach of this method to motion
magnification is the exaggeration of motion by amplifying temporal
color changes at fixed positions, instead of, explicitly estimation
of motion.

This method approach, illustrated in figure~\ref{fig:evm}, combines
spatial and temporal processing to emphasize subtle temporal changes
in a video. First, the video sequence is decomposed into different
spatial frequency bands. Because they may exhibit different
signal-to-noise ratios, they may be magnified differently.
In the general case, the full Laplacian pyramid~\cite{Burt1983Laplacian}
may be computed. Then, temporal processing is performed on each
spatial band. The temporal processing is uniform for all spatial
bands, and for all pixels within each band. After that, the extracted
bandpass signal is magnified by a factor of $\alpha$, which can be
specified by the user, and may be attenuated automatically. Finally,
the magnified signal is added to the original and the spatial pyramid
collapsed to obtain the final output.

\subsubsection{Spatial filtering} \label{sec:sota:post:evm:spatial}

As mention before, the work of~\cite{Wu2012Eulerian} computes the full
Laplacian pyramid~\cite{Burt1983Laplacian} as a general case for spatial
filtering. Each layer of the pyramid may be magnified differently because
it may exhibit different signal-to-noise ratios, or contain spatial frequencies
for which the linear approximation used in motion magnification does not
hold~\cite[section 3]{Wu2012Eulerian}.

Spatial filtering may also be used to significantly increases signal-to-noise
ratio, as previously mention on section~\ref{sec:sota:photo} and demonstrated
on the work of~\cite{Verkruysse2008Remote} and~\cite{Wu2012Eulerian}. Subtle
signals, such as, a person's heart rate from a video of its face, may be
enhanced this way. For this purpose the work of~\cite{Wu2012Eulerian} computes
a layer of the Gaussian pyramid which may be obtained by successively scaling
down the image by calculating the Gaussian average for each pixel.

However, for the signal of interest to be revealed, the spatial filter applied
must be large enough. Section 5 of~\cite{Wu2012Eulerian} provides an equation
to estimate the size for a spatial filter needed to reveal a signal at a
certain noise power level:

\begin{equation}
  S(\lambda) = S(r) = \sigma'^2 = k \frac{\sigma^2}{r^2}
\end{equation}

where $S(\lambda)$ represents the signal over spatial frequencies, and since
the wavelength, $\lambda$, cutoff of a spatial filter is proportional to its
radius, $r$, the signal may be represented as $S(r)$. The noise power,
$\sigma^2$, can be estimated using to the technique of~\cite{Liu2006Noise}.
Finally, because the filtered noise power level, $\sigma'^2$, is inversely
proportional to $r^2$, it is possible to solve the equation for $r$, where
$k$ is a constant that depends on the shape of the low pass filter.

\subsubsection{Temporal filtering} \label{sec:sota:post:evm:temporal}

\fig{temporal-filters}{Examples of temporal filters.}

Temporal filtering is used to extract the motions or signals to be amplified.
Thus, the filter choice is application dependent. For motion magnification,
a broad bandpass filter, such as, the butterworth filter, is preferred. A
narrow bandpass filter produces a more noise-free result for color
amplification of blood flow. An ideal bandpass filter is used
on~\cite{Wu2012Eulerian} due to its sharp cutoff frequencies. Alternatively,
for a real-time implementation low-order IIR filters can be useful for both:
color amplification and motion magnification. These filters are illustrated
on~\ref{fig:temporal-filters}.

\subsubsection{Emphasize color variations for human pulse} \label{sec:sota:evm:color}

% TODO improve section

\fig{evm-color}{Emphasis of face color changes using the \evm{} method.}

The extraction of a person's cardiac pulse using the \evm{} method
was demonstrated in~\cite{Wu2012Eulerian}. It was also presented that
using the right configuration can help extract the desired signal.
There are four steps to take when processing a video using the \evm{} method:

\begin{enumerate}
  \item select a temporal bandpass filter;
  \item select an amplification factor, $\alpha$;
  \item select a spatial frequency cutoff (specified by spatial wavelength,
        $\lambda_c$) beyond which an attenuated version of $\alpha$ is used;
  \item select the form of the attenuation for $\alpha$ —- either force
        $\alpha$ to zero for all $\lambda < \lambda_c$, or linearly scale
        $\alpha$ down to zero.
\end{enumerate}

For human pulse color variation, two temporal filters may be used, first
selecting frequencies within 0.4-4Hz, corresponding to 24-240 beats per
minute~(bpm), then a narrow band of 0.83-1Hz~(50-60 bpm) may be used,
if the extraction of the pulse rate was successful.

To emphasize the color change as much as possible, a large amplification
factor, $\alpha \approx 100$, and spatial frequency cutoff,
$\lambda_c \approx 1000$, is applied. With an attenuation of $\alpha$ to
zero for spatial wavelengths below $\lambda_c$.

The resulting output can be seen in figure~\ref{fig:evm-color}.

\subsection{Detrending} \label{sec:sota:post:detrend}

Detrending is a method of removing very large ultralow-frequency trends an
input signal without any magnitude distortion, acting as an high-pass filter.

The main advantage of the method presented on the work
of~\cite{Tarvainen2002Advanced}, compared to methods presented
in~\cite{Litvack1995Time} and~\cite{Porges1990Analysis}, is its simplicity.

% TODO equation
% TODO image

\section{Heart rate estimation} \label{sec:sota:estimation}

% TODO

\subsection{Power spectrum} \label{sec:sota:estimation:power}

% TODO
% FFT

\subsection{Pulse onset detection} \label{sec:sota:estimation:pulse}

% TODO
% validations

\section{Technologies} \label{sec:sota:tech}

% TODO improve / extend section

Below are short descriptions of the main technologies that will be
used during this research work.

\begin{description}

\item[Android SDK] \hfill \label{sec:sota:tech:android}

\emph{Android SDK} is the development kit for the \emph{Android}
platform. The \emph{Android} platform is an open source, Linux-based
operating system, primarily designed for touchscreen mobile devices,
such as, smartphones.

Because of its open source code and permissive licensing, it allows
the software to be freely modified and distributed. This have allowed
\emph{Android} to be the software of choice for technology companies
who require a low-cost, customizable, and lightweight operating system
for mobile devices and others.

\emph{Android} has also become the world's most widely used smartphone
platform with a worldwide smartphone market share of 75\%
during the third quarter of 2012~\cite{Idc2013Android}.

\item[OpenCV -- Computer Vision Library] \hfill \label{sec:sota:tech:opencv}

\emph{OpenCV} is a library of programming functions mainly aimed at
real-time image processing. To support these, it also includes a
statistical machine learning library. Moreover, it is a cross-platform
and open source library that is free to use and modify under the BSD
license.

\begin{quote}
  ``OpenCV was built to provide a common infrastructure for computer
  vision applications and to accelerate the use of machine perception
  in the commercial products.''~\cite{Opencv2013About}
\end{quote}

\end{description}

\section{Chapter summary}

% TODO
